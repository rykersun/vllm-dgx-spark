#!/usr/bin/env bash
set -euo pipefail

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# DGX Spark vLLM Head Node - Production Setup Script
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Configuration
IMAGE="${IMAGE:-nvcr.io/nvidia/vllm:25.10-py3}"
NAME="${NAME:-ray-head}"
HF_CACHE="${HF_CACHE:-/raid/hf-cache}"
HF_TOKEN="${HF_TOKEN:-}"  # Set via: export HF_TOKEN=hf_xxx
RAY_VERSION="${RAY_VERSION:-2.52.0}"

# Model configuration
MODEL="${MODEL:-openai/gpt-oss-120b}"
TENSOR_PARALLEL="${TENSOR_PARALLEL:-2}"  # Default to 2 for distributed inference
MAX_MODEL_LEN="${MAX_MODEL_LEN:-8192}"   # Context length for Qwen2.5
GPU_MEMORY_UTIL="${GPU_MEMORY_UTIL:-0.90}"  # Can be aggressive with smaller model

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Auto-detect Network Configuration
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Uses ibdev2netdev to discover active InfiniBand/RoCE interfaces.
# The IP address on the IB/RoCE interface can be any valid IP (not limited
# to link-local addresses). We rely on ibdev2netdev output to identify the
# correct network interface for RDMA communication.
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Discover primary RoCE/IB interface using ibdev2netdev
discover_ib_interface() {
  if command -v ibdev2netdev >/dev/null 2>&1; then
    # Get the first active (Up) interface from ibdev2netdev
    local active_line
    active_line=$(ibdev2netdev 2>/dev/null | awk '/\(Up\)/ {print; exit}')

    if [ -n "$active_line" ]; then
      # Extract interface name (5th field, removing parentheses)
      echo "$active_line" | awk '{print $5}' | tr -d '()'
    fi
  fi
}

# Get all active IB/RoCE HCAs (comma-separated for NCCL_IB_HCA)
discover_all_ib_hcas() {
  if command -v ibdev2netdev >/dev/null 2>&1; then
    ibdev2netdev 2>/dev/null | grep "(Up)" | awk '{print $1}' | sort | tr '\n' ',' | sed 's/,$//'
  fi
}

# Get the first IPv4 address from an interface
get_interface_ip() {
  local iface="$1"
  if [ -n "$iface" ]; then
    ip -o addr show "$iface" 2>/dev/null | grep "inet " | awk '{print $4}' | cut -d'/' -f1 | head -1
  fi
}

# Auto-detect primary IB/RoCE interface
PRIMARY_IB_IF=$(discover_ib_interface)

# Auto-detect HEAD_IP from IB interface (or use override)
if [ -z "${HEAD_IP:-}" ]; then
  if [ -n "${PRIMARY_IB_IF}" ]; then
    HEAD_IP=$(get_interface_ip "${PRIMARY_IB_IF}")
  fi
  # Final fallback if auto-detection fails
  if [ -z "${HEAD_IP:-}" ]; then
    echo "ERROR: Could not auto-detect HEAD_IP from InfiniBand/RoCE interface."
    echo ""
    echo "Please ensure:"
    echo "  1. The InfiniBand/RoCE cable is connected between nodes"
    echo "  2. Run 'ibdev2netdev' to verify IB/RoCE interfaces are Up"
    echo "  3. Check that an IP is assigned to the IB/RoCE interface"
    echo ""
    echo "Then either:"
    echo "  - Fix the interface and re-run this script, OR"
    echo "  - Set HEAD_IP manually: export HEAD_IP=<your_ib_ip>"
    exit 1
  fi
fi

# Auto-detect network interfaces from active IB/RoCE devices
if [ -z "${GLOO_IF:-}" ] || [ -z "${TP_IF:-}" ] || [ -z "${NCCL_IF:-}" ] || [ -z "${UCX_DEV:-}" ]; then
  if [ -n "${PRIMARY_IB_IF}" ]; then
    # Use primary IB interface for all NCCL/GLOO/TP/UCX communication
    GLOO_IF="${GLOO_IF:-${PRIMARY_IB_IF}}"
    TP_IF="${TP_IF:-${PRIMARY_IB_IF}}"
    NCCL_IF="${NCCL_IF:-${PRIMARY_IB_IF}}"
    UCX_DEV="${UCX_DEV:-${PRIMARY_IB_IF}}"
  else
    # Error if no IB interface detected and not manually specified
    echo "ERROR: No active InfiniBand/RoCE interface detected."
    echo "Run 'ibdev2netdev' to check interface status."
    exit 1
  fi
fi

# Auto-detect InfiniBand HCAs using ibdev2netdev (or use override)
if [ -z "${NCCL_IB_HCA:-}" ]; then
  IB_DEVICES=$(discover_all_ib_hcas)
  if [ -n "${IB_DEVICES}" ]; then
    NCCL_IB_HCA="${IB_DEVICES}"
  else
    # Fallback: use all IB devices from sysfs
    IB_DEVICES=$(ls -1 /sys/class/infiniband/ 2>/dev/null | tr '\n' ',' | sed 's/,$//')
    NCCL_IB_HCA="${IB_DEVICES:-}"
    if [ -z "${NCCL_IB_HCA}" ]; then
      echo "ERROR: No InfiniBand HCAs detected."
      echo "Run 'ibdev2netdev' or check /sys/class/infiniband/"
      exit 1
    fi
  fi
fi

# Set OMPI_MCA for MPI-based communication (needed for some frameworks)
OMPI_MCA_IF="${NCCL_IF}"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*"
}

error() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $*" >&2
  exit 1
}

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Starting DGX Spark vLLM Head Node Setup"
log "Configuration:"
log "  Image:           ${IMAGE}"
log "  Head IP:         ${HEAD_IP} (auto-detected)"
log "  Model:           ${MODEL}"
log "  Tensor Parallel: ${TENSOR_PARALLEL}"
log "  Ray Version:     ${RAY_VERSION}"
log ""
log "Network Configuration (auto-detected from ibdev2netdev):"
log "  Primary IB IF:   ${PRIMARY_IB_IF:-<not detected>}"
log "  GLOO Interface:  ${GLOO_IF}"
log "  TP Interface:    ${TP_IF}"
log "  NCCL Interface:  ${NCCL_IF}"
log "  UCX Device:      ${UCX_DEV}"
log "  OMPI MCA IF:     ${OMPI_MCA_IF}"
log "  NCCL IB HCAs:    ${NCCL_IB_HCA}"
log ""
if [ -n "${HF_TOKEN}" ]; then
  log "  HF Auth:        âœ… Token provided"
else
  log "  HF Auth:        âš ï¸  No token (gated models will fail)"
fi
log ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 1/10: Pulling Docker image"
if ! docker pull "${IMAGE}"; then
  error "Failed to pull image ${IMAGE}"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 2/10: Cleaning old container"
if docker ps -a --format '{{.Names}}' | grep -qx "${NAME}"; then
  log "  Removing existing container: ${NAME}"
  docker rm -f "${NAME}" >/dev/null
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 3/10: Starting head container"

# Build environment variable args for IB/NCCL configuration
# These are passed into the container to ensure NCCL uses the IB/RoCE link
ENV_ARGS=(
  -e VLLM_HOST_IP="${HEAD_IP}"
  # IB/RoCE interface settings for NCCL communication
  -e GLOO_SOCKET_IFNAME="${GLOO_IF}"
  -e TP_SOCKET_IFNAME="${TP_IF}"
  -e NCCL_SOCKET_IFNAME="${NCCL_IF}"
  -e UCX_NET_DEVICES="${UCX_DEV}"
  -e OMPI_MCA_btl_tcp_if_include="${OMPI_MCA_IF}"
  # NCCL InfiniBand settings
  -e NCCL_IB_DISABLE=0
  -e NCCL_IB_HCA="${NCCL_IB_HCA}"
  -e NCCL_NET_GDR_LEVEL=5
  # Debug settings (can be disabled for production by setting NCCL_DEBUG=WARN)
  -e NCCL_DEBUG="${NCCL_DEBUG:-INFO}"
  -e NCCL_DEBUG_SUBSYS="${NCCL_DEBUG_SUBSYS:-INIT,NET}"
  # NVIDIA/GPU settings
  -e NVIDIA_VISIBLE_DEVICES=all
  -e NVIDIA_DRIVER_CAPABILITIES=all
  # Ray settings
  -e RAY_memory_usage_threshold=0.998
  -e RAY_GCS_SERVER_PORT=6380
  # HuggingFace cache
  -e HF_HOME=/root/.cache/huggingface
)

# Add HuggingFace token if provided
if [ -n "${HF_TOKEN}" ]; then
  ENV_ARGS+=(-e HF_TOKEN="${HF_TOKEN}")
fi

docker run -d \
  --restart unless-stopped \
  --name "${NAME}" \
  --gpus all \
  --network host \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  --device=/dev/infiniband \
  -v "${HF_CACHE}:/root/.cache/huggingface" \
  "${ENV_ARGS[@]}" \
  "${IMAGE}" sleep infinity

if ! docker ps | grep -q "${NAME}"; then
  error "Container failed to start"
fi

log "  Container started successfully"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 4/10: Installing RDMA/InfiniBand libraries for NCCL"
log "  These libraries are required for NCCL to use InfiniBand/RoCE instead of Ethernet"
if ! docker exec "${NAME}" bash -lc "
  apt-get update -qq >/dev/null 2>&1
  apt-get install -y -qq \
    infiniband-diags \
    libibverbs1 \
    librdmacm1 \
    rdma-core \
    ibverbs-providers \
    >/dev/null 2>&1
"; then
  log "  âš ï¸  Warning: Could not install RDMA libraries (may already be present)"
fi

# Verify RDMA libraries are available
if docker exec "${NAME}" bash -lc "ldconfig -p 2>/dev/null | grep -q libibverbs"; then
  log "  âœ… RDMA libraries installed (libibverbs, librdmacm)"
else
  log "  âš ï¸  Warning: RDMA libraries may not be properly installed"
  log "     NCCL may fall back to Socket transport (slower)"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 5/10: Installing Ray ${RAY_VERSION}"
if ! docker exec "${NAME}" bash -lc "pip install -q -U --root-user-action=ignore 'ray==${RAY_VERSION}'"; then
  error "Failed to install Ray"
fi

# Verify Ray version
INSTALLED_RAY_VERSION=$(docker exec "${NAME}" python3 -c "import ray; print(ray.__version__)" 2>/dev/null || echo "unknown")
if [ "${INSTALLED_RAY_VERSION}" != "${RAY_VERSION}" ]; then
  error "Ray version mismatch: expected ${RAY_VERSION}, got ${INSTALLED_RAY_VERSION}"
fi

log "  Ray ${INSTALLED_RAY_VERSION} installed"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 6/10: Pre-downloading model weights"
log "  Model: ${MODEL}"
log "  This may take a while for large models on first download..."

# Build HF token arg if provided
HF_TOKEN_ARG=""
if [ -n "${HF_TOKEN}" ]; then
  HF_TOKEN_ARG="--token ${HF_TOKEN}"
fi

# Download model with verification
if ! docker exec "${NAME}" bash -lc "
  export HF_HOME=/root/.cache/huggingface
  echo '  Downloading model files (excluding original/* and metal/* to save space)...'
  hf download ${MODEL} ${HF_TOKEN_ARG} --exclude 'original/*' --exclude 'metal/*' 2>&1 | tail -5
"; then
  error "Failed to download model ${MODEL}"
fi

# Verify model was downloaded by checking for config.json
if ! docker exec "${NAME}" bash -lc "
  export HF_HOME=/root/.cache/huggingface
  python3 -c \"
from huggingface_hub import snapshot_download
import os
path = snapshot_download('${MODEL}', local_files_only=True)
config_path = os.path.join(path, 'config.json')
if not os.path.exists(config_path):
    raise FileNotFoundError(f'Model config not found at {config_path}')
print(f'  âœ… Model verified at: {path}')
\"
"; then
  error "Model verification failed - config.json not found"
fi

log "  Model download complete and verified"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 7/10: Starting Ray head"
docker exec "${NAME}" bash -lc "
  ray stop --force 2>/dev/null || true
  ray start --head \
    --node-ip-address=${HEAD_IP} \
    --port=6380 \
    --dashboard-host=0.0.0.0 \
    --dashboard-port=8265
" >/dev/null

log "  Ray head started, waiting for readiness..."

# Wait for Ray to become ready
for i in {1..30}; do
  if docker exec "${NAME}" bash -lc "ray status --address='127.0.0.1:6380' >/dev/null 2>&1"; then
    log "  âœ… Ray head is ready (${i}s)"
    break
  fi
  if [ $i -eq 30 ]; then
    error "Ray head failed to become ready after 30 seconds"
  fi
  sleep 1
done

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 8/10: Waiting for worker nodes"
log ""
log "  âš ï¸  IMPORTANT: Before proceeding, ensure all worker nodes have:"
log "     1. Downloaded the model: export MODEL=${MODEL} && bash start_worker_vllm.sh"
log "     2. Joined the Ray cluster"
log ""
log "  Checking Ray cluster status..."

# Show current cluster status
docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:6380 2>/dev/null | head -15" || true

CURRENT_NODES=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:6380 2>/dev/null | grep -E '^ [0-9]+ node' | awk '{print \$1}'" 2>/dev/null || echo "1")
CURRENT_GPUS=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:6380 2>/dev/null | grep 'GPU:' | awk -F'/' '{print \$2}' | awk '{print \$1}'" 2>/dev/null || echo "1")

log ""
log "  Current cluster: ${CURRENT_NODES} node(s), ${CURRENT_GPUS} GPU(s)"

if [ "${TENSOR_PARALLEL}" -gt "${CURRENT_GPUS:-1}" ]; then
  log ""
  log "  âš ï¸  Warning: tensor-parallel-size (${TENSOR_PARALLEL}) > available GPUs (${CURRENT_GPUS})"
  log "     Waiting 30 seconds for worker nodes to join..."
  log "     (Press Ctrl+C to abort and add workers manually)"

  for i in {1..30}; do
    CURRENT_GPUS=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:6380 2>/dev/null | grep 'GPU:' | awk -F'/' '{print \$2}' | awk '{print \$1}'" 2>/dev/null || echo "1")
    if [ "${CURRENT_GPUS:-1}" -ge "${TENSOR_PARALLEL}" ]; then
      log "  âœ… Sufficient GPUs available: ${CURRENT_GPUS}"
      break
    fi
    if [ $i -eq 30 ]; then
      log "  âš ï¸  Proceeding with ${CURRENT_GPUS} GPU(s) - vLLM may fail if insufficient"
    fi
    sleep 1
  done
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 9/10: Starting vLLM server"
log ""

# Kill any existing vLLM processes
docker exec "${NAME}" bash -lc "pkill -f 'vllm serve' 2>/dev/null || true" || true

log "  Starting vLLM in background (this launches the server process)..."

# Start vLLM in background using nohup
# Note: We do NOT set HF_HUB_OFFLINE=1 here because workers need to resolve the model name
# The model should already be downloaded on all nodes via the pre-download step
docker exec "${NAME}" bash -lc "
  export HF_HOME=/root/.cache/huggingface
  export RAY_ADDRESS=127.0.0.1:6380
  export PYTHONUNBUFFERED=1
  export VLLM_LOGGING_LEVEL=INFO

  nohup vllm serve ${MODEL} \
    --distributed-executor-backend ray \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size ${TENSOR_PARALLEL} \
    --max-model-len ${MAX_MODEL_LEN} \
    --gpu-memory-utilization ${GPU_MEMORY_UTIL} \
    --download-dir \$HF_HOME \
    > /var/log/vllm.log 2>&1 &

  sleep 1
" || true

log "  vLLM server process started"
log ""
log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
log "  ğŸ”„ MODEL LOADING IN PROGRESS"
log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
log ""
log "  This process typically takes 2-5 minutes depending on model size."
log "  The server will:"
log "    1. Load model weights into GPU memory"
log "    2. Initialize tensor parallelism across GPUs"
log "    3. Compile CUDA graphs for optimized inference"
log ""
log "  Progress updates will appear below..."
log ""

# Wait for vLLM to become ready with detailed progress feedback
VLLM_READY=false
MAX_WAIT=600  # 10 minutes max for very large models
LAST_STATUS=""
START_TIME=$(date +%s)

for i in $(seq 1 $MAX_WAIT); do
  CURRENT_TIME=$(date +%s)
  ELAPSED=$((CURRENT_TIME - START_TIME))
  MINS=$((ELAPSED / 60))
  SECS=$((ELAPSED % 60))

  # Check if vLLM is ready
  if docker exec "${NAME}" bash -lc "curl -sf http://127.0.0.1:8000/health >/dev/null 2>&1"; then
    echo ""
    log ""
    log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    log "  âœ… MODEL LOADED SUCCESSFULLY!"
    log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    log ""
    log "  vLLM is ready and accepting requests (loaded in ${MINS}m ${SECS}s)"
    log ""
    VLLM_READY=true
    break
  fi

  # Check vLLM process status and extract progress from logs
  VLLM_PID=$(docker exec "${NAME}" bash -lc "pgrep -f 'vllm serve' 2>/dev/null" || echo "")

  if [ -z "${VLLM_PID}" ]; then
    # vLLM process died - check logs for error
    echo ""
    log ""
    log "  âŒ vLLM process exited unexpectedly!"
    log ""
    log "  Last 20 lines of vLLM log:"
    docker exec "${NAME}" tail -20 /var/log/vllm.log 2>/dev/null || true
    log ""
    error "vLLM failed to start. Check logs: docker exec ${NAME} cat /var/log/vllm.log"
  fi

  # Parse last meaningful log line to show progress
  CURRENT_STATUS=$(docker exec "${NAME}" bash -lc "tail -50 /var/log/vllm.log 2>/dev/null | grep -E '(Loading|Loaded|weight|layer|graph|CUDA|tensor|parallel|shard|download|progress|INFO)' | tail -1 | sed 's/.*INFO/INFO/' | cut -c1-80" 2>/dev/null || echo "")

  # Show spinner with elapsed time
  SPINNER="â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â "
  SPIN_CHAR="${SPINNER:$((i % 10)):1}"

  # Update progress display
  printf "\r  %s Loading model... [%dm %02ds elapsed]  " "${SPIN_CHAR}" "${MINS}" "${SECS}"

  # Show status updates when they change (avoid flooding terminal)
  if [ -n "${CURRENT_STATUS}" ] && [ "${CURRENT_STATUS}" != "${LAST_STATUS}" ]; then
    echo ""
    log "     ${CURRENT_STATUS}"
    LAST_STATUS="${CURRENT_STATUS}"
  fi

  # Periodic milestone messages
  if [ $((i % 60)) -eq 0 ]; then
    echo ""
    log "  â³ Still loading... (${MINS}m ${SECS}s) - this is normal for large models"
  fi

  sleep 1
done

# Handle timeout
if [ "${VLLM_READY}" != "true" ]; then
  echo ""
  log ""
  log "  âš ï¸  vLLM not ready after $((MAX_WAIT / 60)) minutes"
  log ""
  log "  This could mean:"
  log "    - Model is still loading (very large models may need more time)"
  log "    - An error occurred during loading"
  log ""
  log "  Check the logs for details:"
  log "    docker exec ${NAME} tail -100 /var/log/vllm.log"
  log ""
  log "  You can also monitor GPU memory to see if loading is progressing:"
  log "    watch -n 1 nvidia-smi"
  log ""
fi

log ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 10/10: Running health checks"

# Check Ray status
RAY_NODES=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:6380 2>/dev/null | grep 'Healthy:' -A1 | tail -1 | awk '{print \$1}'" || echo "0")
log "  Ray cluster: ${RAY_NODES} node(s) healthy"

# Check vLLM models
VLLM_MODEL=$(docker exec "${NAME}" bash -lc "curl -sf http://127.0.0.1:8000/v1/models 2>/dev/null | python3 -c 'import sys,json; print(json.load(sys.stdin)[\"data\"][0][\"id\"])' 2>/dev/null" || echo "unknown")
log "  vLLM model: ${VLLM_MODEL}"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Detect public-facing IP for user access (exclude loopback and docker bridge)
PUBLIC_IP=$(ip -o addr show | grep "inet " | grep -v "127.0.0.1" | grep -v "172.17" | awk '{print $4}' | cut -d'/' -f1 | head -1)
if [ -z "${PUBLIC_IP}" ]; then
  PUBLIC_IP="${HEAD_IP}"  # Fallback to IB IP if no other found
fi

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Head node is ready!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸŒ Services (accessible from network):"
echo "  Ray Dashboard:  http://${PUBLIC_IP}:8265"
echo "  vLLM API:       http://${PUBLIC_IP}:8000"
echo ""
echo "ğŸ”— Next Steps - Add Worker Nodes:"
echo "  1. SSH to each worker node"
echo "  2. Run: export HEAD_IP=${HEAD_IP}"
echo "  3. Run: bash start_worker_vllm.sh"
echo ""
echo "  Note: Workers use IB/RoCE IP (${HEAD_IP}) for cluster communication"
echo "  Note: Worker IPs and network interfaces will be auto-detected!"
echo ""
echo "ğŸ“Š Quick API Tests:"
echo "  # List models"
echo "  curl http://${PUBLIC_IP}:8000/v1/models"
echo ""
echo "  # Chat completion"
echo "  curl http://${PUBLIC_IP}:8000/v1/chat/completions \\"
echo "    -H 'Content-Type: application/json' \\"
echo "    -d '{\"model\":\"${MODEL}\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello\"}]}'"
echo ""
echo "ğŸ” Monitoring Commands:"
echo "  # View vLLM logs"
echo "  docker exec ${NAME} tail -f /var/log/vllm.log"
echo ""
echo "  # Ray cluster status (check for worker nodes)"
echo "  docker exec ${NAME} ray status --address=127.0.0.1:6380"
echo ""
echo "  # GPU utilization"
echo "  watch -n 1 nvidia-smi"
echo ""
echo "âš™ï¸  Current Configuration:"
echo "  Model:              ${MODEL}"
echo "  Tensor Parallelism: ${TENSOR_PARALLEL} GPUs"
echo "  Max Context:        ${MAX_MODEL_LEN} tokens"
echo "  GPU Memory:         ${GPU_MEMORY_UTIL} utilization"
echo "  CUDA Graphs:        Enabled (optimized for performance)"
echo ""
echo "ğŸ“Š Expected Performance:"
echo "  Throughput:         50-100 tokens/second"
echo "  First request:      May be slower (graph warmup)"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
