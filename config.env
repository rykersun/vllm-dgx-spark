# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# vLLM DGX Spark Cluster Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Copy this file to config.local.env and customize for your environment.
# config.local.env is gitignored and won't be committed.
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Required for Multi-Node                                                 │
# └─────────────────────────────────────────────────────────────────────────┘
# Worker node InfiniBand IP (run 'ibdev2netdev' on worker to find it)
WORKER_IPS="${WORKER_IPS:-}"

# SSH username for worker nodes
WORKER_USER="${WORKER_USER:-$(whoami)}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Model Settings                                                          │
# └─────────────────────────────────────────────────────────────────────────┘
# Model to serve (HuggingFace model ID)
MODEL="${MODEL:-openai/gpt-oss-120b}"

# Tensor parallelism (total GPUs: 1 per node × number of nodes)
TENSOR_PARALLEL="${TENSOR_PARALLEL:-2}"

# GPU memory utilization for KV cache (0.0-1.0)
GPU_MEMORY_UTIL="${GPU_MEMORY_UTIL:-0.90}"

# Maximum context length (tokens)
MAX_MODEL_LEN="${MAX_MODEL_LEN:-8192}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Docker Settings                                                         │
# └─────────────────────────────────────────────────────────────────────────┘
# vLLM Docker image
VLLM_IMAGE="${VLLM_IMAGE:-nvcr.io/nvidia/vllm:25.11-py3}"

# Container names
HEAD_CONTAINER_NAME="${HEAD_CONTAINER_NAME:-ray-head}"
WORKER_CONTAINER_NAME="${WORKER_CONTAINER_NAME:-ray-worker}"

# Shared memory size
SHM_SIZE="${SHM_SIZE:-16g}"

# Ray version (must match between head and workers)
# Container has 2.51.1, we upgrade to 2.52.1 for latest features
RAY_VERSION="${RAY_VERSION:-2.52.1}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Network Settings                                                        │
# └─────────────────────────────────────────────────────────────────────────┘
# vLLM API port
VLLM_PORT="${VLLM_PORT:-8000}"

# Ray dashboard port
RAY_DASHBOARD_PORT="${RAY_DASHBOARD_PORT:-8265}"

# Ray GCS port
RAY_PORT="${RAY_PORT:-6380}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Storage Settings                                                        │
# └─────────────────────────────────────────────────────────────────────────┘
# HuggingFace cache directory (should be same path on all nodes)
HF_CACHE="${HF_CACHE:-/raid/hf-cache}"

# Worker's HF cache (defaults to same as head)
WORKER_HF_CACHE="${WORKER_HF_CACHE:-${HF_CACHE}}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Authentication                                                          │
# └─────────────────────────────────────────────────────────────────────────┘
# HuggingFace token (required for gated models like Llama, Gemma)
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN="${HF_TOKEN:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ vLLM Options                                                            │
# └─────────────────────────────────────────────────────────────────────────┘
# Enable expert parallelism for MoE models
ENABLE_EXPERT_PARALLEL="${ENABLE_EXPERT_PARALLEL:-true}"

# Swap space in GB
SWAP_SPACE="${SWAP_SPACE:-16}"

# Trust remote code (required for some models like Phi-4)
TRUST_REMOTE_CODE="${TRUST_REMOTE_CODE:-false}"

# Model loading format: auto, safetensors, fastsafetensors
# fastsafetensors uses GPU Direct Storage for faster loading (installed automatically)
LOAD_FORMAT="${LOAD_FORMAT:-fastsafetensors}"

# Additional vLLM arguments
EXTRA_ARGS="${EXTRA_ARGS:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ NCCL/InfiniBand Settings (auto-detected, override if needed)            │
# └─────────────────────────────────────────────────────────────────────────┘
# These are typically auto-detected from ibdev2netdev
# Uncomment to override:
# NCCL_IB_DISABLE=0
# NCCL_IB_HCA=mlx5_0,mlx5_1
# NCCL_SOCKET_IFNAME=enp1s0f1np1
# NCCL_NET_GDR_LEVEL=5

# Debug settings (set to INFO for troubleshooting)
NCCL_DEBUG="${NCCL_DEBUG:-WARN}"
# NCCL_DEBUG_SUBSYS=INIT,NET
