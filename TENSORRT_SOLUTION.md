# TensorRT-LLM Solution for DGX Spark - Detailed Implementation Plan

## Executive Summary

After deep investigation into TensorRT-LLM's codebase and issues, I've identified **concrete solutions** to make it work on your two DGX Spark servers. The key findings:

### ‚úÖ **Good News**

1. **SM120 support IS available** - merged to main branch via PR #7937 (Oct 6, 2025)
2. **CUDA graph hang IS fixed** - PR #8803 merged (Nov 2, 2025)
3. **Multiple SM120 optimizations** exist in main branch:
   - PR #9054: KV cache memory optimization for SM120
   - PR #8944: Eagle3 accuracy fix for SM120
   - PR #8844: FP8 blockscaled GEMM for SM120
   - PR #8620: nvfp4 CUDA core support for SM120

### ‚ö†Ô∏è **The Challenge**

**These fixes are NOT in v1.2.0rc3** (the latest pre-built container)

Latest GitHub release is v1.2.0rc2 (Nov 7, 2024) - over a year old!
Main branch HEAD is current as of Nov 11, 2025 (commit `aca56097`)

### üîß **The Solution**

**Build TensorRT-LLM from main branch** to get all SM120 fixes

---

## Solution Approaches

### Approach 1: Build from Source (Recommended)

**Goal**: Compile TensorRT-LLM from main branch with all SM120 fixes

**Pros**:
- ‚úÖ Gets all latest SM120 kernel support
- ‚úÖ Includes CUDA graph hang fix (PR #8803)
- ‚úÖ All memory optimizations for GB10
- ‚úÖ Tested and merged code (not experimental)

**Cons**:
- ‚è±Ô∏è Build time: 2-4 hours on DGX Spark
- üíæ Requires 63GB disk space
- üîß Complex build process on ARM

**Risk Level**: Medium

**Expected Outcome**: TensorRT-LLM working with SM120

### Approach 2: Wait for v1.2.0 Final Release

**Timeline**: Unknown (no announced date)

**Pros**:
- ‚úÖ Pre-built binaries
- ‚úÖ Official support
- ‚úÖ Tested release

**Cons**:
- ‚è≥ Could be weeks or months
- ‚ö†Ô∏è No guarantee SM120 will be in v1.2.0 final
- üö´ Can't use TensorRT-LLM now

**Risk Level**: None (but no solution)

### Approach 3: Use NGC Latest Container (if available)

**Check if NGC has newer builds** beyond v1.2.0rc3

**To verify**:
```bash
docker pull nvcr.io/nvidia/tensorrt-llm/release:latest
docker run --rm nvcr.io/nvidia/tensorrt-llm/release:latest python3 -c "import tensorrt_llm; print(tensorrt_llm.__version__)"
```

**If version > v1.2.0rc3**: May have SM120 fixes

**Risk Level**: Low

---

## Detailed Build from Source Plan

### Prerequisites Check

**Your System** (verified):
- ‚úÖ CUDA 13.0.88 (required: CUDA 13.0+)
- ‚úÖ Driver 580.95.05 (compatible)
- ‚úÖ GB10 GPU (SM120/12.1)
- ‚úÖ ARM aarch64 architecture
- ‚úÖ Ubuntu 24.04
- ‚úÖ Docker installed

**Additional Requirements**:
- Git LFS (for large model files)
- ~63GB free disk space
- 2-4 hours build time

### Build Strategy

#### Option A: Docker Build (Recommended)

**Pros**: Isolated environment, reproducible build, includes all dependencies

**Steps**:
1. Clone TensorRT-LLM main branch
2. Build Docker image with SM120 support
3. Launch container for testing
4. Deploy to both DGX Sparks

#### Option B: Native Build (Advanced)

**Pros**: Faster iteration, no Docker overhead

**Cons**: May have dependency conflicts with system packages

### Multi-Node Configuration

Based on code analysis of `examples/run.py` and resolved issues:

**Key Requirements**:
1. **MPI** - TensorRT-LLM uses MPI for multi-node coordination
2. **Shared filesystem** or **model sync** - Both nodes need access to engine files
3. **Network** - Use InfiniBand IPs (already configured)
4. **NCCL** - Already working with InfiniBand (from vLLM testing)

**Execution Pattern**:
```bash
# On head node (rank 0)
mpirun -n 2 -H localhost:1,<worker_ip>:1 \
  python3 run.py --engine_dir ./engines/llama-70b-tp2 ...

# rank 0 (head): Handles model loading, input, output
# rank 1 (worker): Participates in distributed inference
```

---

## Implementation: Build from Main Branch

### Phase 1: Preparation

**1.1 Check Disk Space**
```bash
df -h /
# Need: 63GB free for build
```

**1.2 Install Git LFS**
```bash
sudo apt-get update
sudo apt-get install -y git git-lfs
git lfs install
```

**1.3 Clone Repository**
```bash
cd ~
git clone https://github.com/NVIDIA/TensorRT-LLM.git
cd TensorRT-LLM
git checkout main  # Ensure we're on main branch
git submodule update --init --recursive
git lfs pull
```

**1.4 Verify Commit**
```bash
git log -1 --oneline
# Should show recent commit (Nov 11, 2025 or later)
# Expected: aca56097 or newer
```

### Phase 2: Build Docker Image

**2.1 Build Development Image**

**For SM120 support**, we need to specify CUDA architecture:

```bash
cd ~/TensorRT-LLM

# Build with SM120 (compute capability 12.1)
make -C docker build CUDA_ARCHS="120-real"
```

**Alternative - Build release image**:
```bash
make -C docker release_build CUDA_ARCHS="120-real"
```

**Build time**: 2-4 hours on DGX Spark

**What this does**:
- Downloads base PyTorch container (nvcr.io/nvidia/pytorch:25.10-py3)
- Installs all dependencies
- Compiles C++ kernels with SM120 support
- Builds Python wheel
- Installs TensorRT-LLM with all SM120 optimizations

**2.2 Monitor Build**
```bash
# Build runs in foreground, watch for errors
# Key indicators of success:
#   - "Building wheel for tensorrt_llm"
#   - "Successfully built tensorrt_llm"
#   - No "ERROR" or "FAILED" messages
```

**2.3 Verify Build Success**
```bash
docker images | grep tensorrt_llm
# Should show: tensorrt_llm/devel:latest or tensorrt_llm/release:latest
```

### Phase 3: Test Single-Node First

**3.1 Launch Container**
```bash
docker run --rm -it \
  --gpus all \
  --ipc host \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  -p 8000:8000 \
  --name trtllm-test \
  tensorrt_llm/release:latest
```

**3.2 Verify SM120 Support**
```python
# Inside container
python3 << 'EOF'
import tensorrt_llm
print(f"TensorRT-LLM version: {tensorrt_llm.__version__}")

# Check if SM120 kernels are available
from tensorrt_llm import bindings
print(f"C++ bindings available: {hasattr(bindings, 'GptSession')}")
EOF
```

**3.3 Test with Small Model**
```bash
# Inside container
# Use trtllm-serve with Llama-3.1-8B
trtllm-serve meta-llama/Llama-3.1-8B-Instruct
```

**Success criteria**:
- Server starts without errors
- No "does not support SM120" errors
- Model loads successfully
- Can generate tokens

**If successful**: SM120 support is working! ‚úÖ

### Phase 4: Build Engines for Multi-Node

**4.1 Build Llama-70B Engine with TP=2**

```bash
# Inside container
cd /app/tensorrt_llm/examples/llama

# Convert HuggingFace checkpoint to TensorRT-LLM format
python3 convert_checkpoint.py \
  --model_dir /path/to/Llama-3.3-70B-Instruct \
  --output_dir ./llama-70b-ckpt \
  --tp_size 2 \
  --dtype float16

# Build TensorRT engines (one per GPU)
trtllm-build \
  --checkpoint_dir ./llama-70b-ckpt \
  --output_dir ./llama-70b-engines \
  --gemm_plugin float16 \
  --max_batch_size 8 \
  --max_input_len 2048 \
  --max_seq_len 4096 \
  --tp_size 2 \
  --workers 2
```

**Build time**: 20-60 minutes

**Output**: Engine files for rank 0 and rank 1

**4.2 Copy Engines to Both Nodes**

```bash
# On head node
docker cp trtllm-test:/app/tensorrt_llm/examples/llama/llama-70b-engines ./

# Copy to worker node
scp -r ./llama-70b-engines 192.168.7.111:~/
```

### Phase 5: Multi-Node Deployment

**5.1 Install MPI on Both Nodes**

```bash
# On BOTH head and worker
sudo apt-get install -y openmpi-bin openmpi-common libopenmpi-dev
```

**5.2 Setup SSH Keys** (if not already done)

```bash
# On head node
ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
ssh-copy-id 192.168.7.111
```

**5.3 Test MPI**

```bash
# On head node
mpirun -n 2 -H localhost:1,192.168.7.111:1 hostname
# Should show both hostnames
```

**5.4 Launch Multi-Node Inference**

**Option A: Using Docker on Both Nodes**

```bash
# On HEAD node (192.168.7.x):
docker run -d --name trtllm-head \
  --gpus '"device=0"' \
  --ipc host \
  --network host \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  -v $HOME/llama-70b-engines:/engines \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e NCCL_IB_DISABLE=0 \
  -e NCCL_DEBUG=INFO \
  tensorrt_llm/release:latest \
  sleep infinity

# On WORKER node (192.168.7.111):
docker run -d --name trtllm-worker \
  --gpus '"device=0"' \
  --ipc host \
  --network host \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  -v $HOME/llama-70b-engines:/engines \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e NCCL_IB_DISABLE=0 \
  -e NCCL_DEBUG=INFO \
  tensorrt_llm/release:latest \
  sleep infinity
```

**Option B: Using MPI to Launch**

```bash
# Create hostfile
cat > ~/mpi_hostfile << EOF
localhost slots=1
192.168.7.111 slots=1
EOF

# Launch distributed inference
mpirun -n 2 \
  --hostfile ~/mpi_hostfile \
  --mca btl_tcp_if_include <infiniband_interface> \
  docker exec trtllm-head python3 /app/tensorrt_llm/examples/run.py \
    --engine_dir /engines \
    --tokenizer_dir meta-llama/Llama-3.3-70B-Instruct \
    --max_output_len 100 \
    --input_text "What is the capital of France?"
```

---

## Avoiding Common Pitfalls

### Issue 1: CUDA Graph Hangs (FIXED in main)

**Problem**: PR #8781 - hanging with CUDA graphs + AllReduce

**Solution**: Already fixed in PR #8803 (merged Nov 2, 2025)

**Verification**:
```bash
# Check if fix is in your build
cd ~/TensorRT-LLM
git log --oneline --grep="8803"
# Should show: "[#8781][fix] Cache the AllReduce wrapper"
```

**If still encountering hangs**, disable CUDA graphs temporarily:
```bash
export TRTLLM_DISABLE_CUDAGRAPH=1
```

### Issue 2: SM120 Kernel Not Found

**Problem**: Missing SM120 kernels (PR #7937)

**Solution**: Ensure CUDA_ARCHS="120-real" was used during build

**Verification**:
```bash
# Inside container, check compiled architectures
python3 << 'EOF'
from tensorrt_llm._utils import torch_to_numpy
import torch
print(f"CUDA architectures: {torch.cuda.get_arch_list()}")
EOF
```

**Should include**: `sm_120` or `compute_120`

### Issue 3: Double Free Memory Errors (Issue #2953)

**Status**: Still OPEN, but less frequent with latest commits

**Workaround**:
- Use smaller batch sizes
- Monitor with: `docker logs -f <container> | grep "double free"`
- If detected, reduce `--max_batch_size` in trtllm-build

### Issue 4: Engine Build Failures

**Problem**: TensorRT engine compilation fails

**Common causes**:
1. Out of memory during build
2. Incompatible quantization settings
3. Missing model files

**Solutions**:
```bash
# 1. Reduce GPU memory during build
export CUDA_VISIBLE_DEVICES=0  # Use only one GPU

# 2. Use simpler quantization
# Avoid FP8 if causing issues, use FP16 instead

# 3. Verify model download
ls -lh /path/to/model/*.safetensors
# Should show all model shards
```

---

## Performance Expectations

### Build from Main Branch

If all SM120 optimizations are working:

| Configuration | Expected Performance | Comparison to vLLM |
|---------------|---------------------|-------------------|
| Llama-70B, TP=2, FP16 | 8-15 t/s | 3-5x faster |
| Llama-70B, TP=2, FP8 | 15-25 t/s | 5-8x faster |
| Llama-8B, single GPU, FP16 | 100-150 t/s | 2-3x faster |
| Llama-8B, single GPU, FP8 | 200-300 t/s | 4-6x faster |

**Note**: These are theoretical maximums. Actual performance depends on:
- Prompt length
- Generation length
- Batch size
- Network latency (even with InfiniBand)

### If Performance is Still Low

**Diagnostics**:
```bash
# 1. Check GPU utilization
nvidia-smi dmon -s u -d 1

# 2. Check NCCL is using InfiniBand
grep "NET/IB" /var/log/*.log

# 3. Profile with nsys
nsys profile --trace cuda,nvtx \
  python3 run.py --engine_dir /engines ...
```

---

## Rollback Plan

If build from source doesn't work:

### Option 1: Return to vLLM
```bash
# Stop TensorRT-LLM containers
docker stop trtllm-head trtllm-worker
docker rm trtllm-head trtllm-worker

# Restart vLLM (known working)
cd ~/vllm-dgx-spark
./start_head_vllm.sh
# On worker: ./start_worker_vllm.sh
```

### Option 2: Wait for Official Release

Monitor: https://github.com/NVIDIA/TensorRT-LLM/releases

When v1.2.0 final (or v1.3.0) is released with SM120 support:
- Download pre-built container
- Much easier deployment
- Officially supported

---

## Decision Matrix

### Build from Source If:
- ‚úÖ You have 4+ hours for build time
- ‚úÖ You have 63GB+ free disk space
- ‚úÖ You're comfortable with Docker builds
- ‚úÖ You need TensorRT-LLM performance NOW
- ‚úÖ You can dedicate time to debugging

### Wait for Release If:
- ‚è≥ You can wait weeks/months
- ‚úÖ You prefer official support
- ‚úÖ You want pre-built binaries
- ‚ö†Ô∏è Current vLLM performance is acceptable

### Stay with vLLM If:
- ‚úÖ Current performance is sufficient (3-15 t/s with batching)
- ‚úÖ Stability is more important than speed
- ‚úÖ You don't want to invest time in builds
- ‚úÖ Production systems can't afford downtime

---

## Next Steps

1. **Review this document** - Understand the build process
2. **Check disk space** - Ensure 63GB+ available
3. **Decide approach** - Build from source vs wait vs stay with vLLM
4. **If building**: Follow Phase 1-5 in sequence
5. **If waiting**: Monitor GitHub releases
6. **If staying with vLLM**: Optimize with concurrent requests

---

## Support & Monitoring

### Monitor TensorRT-LLM Progress

**GitHub Issues to Watch**:
- #8474 (SM120 support) - CLOSED (merged)
- #8781 (CUDA graph hang) - CLOSED (fixed)
- #2953 (multi-node memory) - OPEN

**GitHub Releases**:
- https://github.com/NVIDIA/TensorRT-LLM/releases
- Watch for v1.2.0 final or v1.3.0

### Community Support

- GitHub Discussions: https://github.com/NVIDIA/TensorRT-LLM/discussions
- NVIDIA Developer Forums: https://forums.developer.nvidia.com/

---

## Conclusion

**TensorRT-LLM CAN work on DGX Spark**, but requires:
1. Building from main branch (not using v1.2.0rc3)
2. Proper SM120 architecture specification during build
3. Careful multi-node MPI configuration
4. InfiniBand network setup (already done)

**Recommended Action**:
- **Short-term**: Continue with vLLM (stable, working)
- **Medium-term**: Build from source when you have time
- **Long-term**: Switch to TensorRT-LLM when v1.2.0 final is released

**Expected Outcome if Built from Source**:
- 3-5x faster than current vLLM setup
- Higher complexity and maintenance burden
- Requires ongoing monitoring of main branch for fixes

The choice depends on your priorities: **stability vs performance**.
