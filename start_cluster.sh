#!/usr/bin/env bash
set -euo pipefail

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# vLLM Cluster Startup Script
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Starts vLLM/Ray on head node and orchestrates worker nodes via SSH.
# Run from head node - it will handle workers automatically.
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Load configuration (config.local.env overrides config.env)
if [ -f "${SCRIPT_DIR}/config.local.env" ]; then
  source "${SCRIPT_DIR}/config.local.env"
elif [ -f "${SCRIPT_DIR}/config.env" ]; then
  source "${SCRIPT_DIR}/config.env"
fi

# Configuration (can be overridden by config.env or environment)
IMAGE="${VLLM_IMAGE:-${IMAGE:-nvcr.io/nvidia/vllm:25.11-py3}}"
NAME="${HEAD_CONTAINER_NAME:-${NAME:-ray-head}}"
HF_CACHE="${HF_CACHE:-/raid/hf-cache}"
HF_TOKEN="${HF_TOKEN:-}"
RAY_VERSION="${RAY_VERSION:-2.52.1}"

# Worker node configuration (for orchestrated setup)
# WORKER_IPS (or legacy WORKER_HOST) must be set to enable automatic worker setup
WORKER_HOST="${WORKER_IPS:-${WORKER_HOST:-}}"  # Support both new and legacy names
WORKER_USER="${WORKER_USER:-$(whoami)}"
WORKER_HF_CACHE="${WORKER_HF_CACHE:-${HF_CACHE}}"

# Model configuration
MODEL="${MODEL:-openai/gpt-oss-120b}"
TENSOR_PARALLEL="${TENSOR_PARALLEL:-2}"
MAX_MODEL_LEN="${MAX_MODEL_LEN:-8192}"
GPU_MEMORY_UTIL="${GPU_MEMORY_UTIL:-0.90}"
SWAP_SPACE="${SWAP_SPACE:-16}"
SHM_SIZE="${SHM_SIZE:-16g}"
ENABLE_EXPERT_PARALLEL="${ENABLE_EXPERT_PARALLEL:-true}"
TRUST_REMOTE_CODE="${TRUST_REMOTE_CODE:-false}"
# Note: fastsafetensors requires --cap-add=SYS_NICE for get_mempolicy syscall
# Default to safetensors which works without special permissions
LOAD_FORMAT="${LOAD_FORMAT:-safetensors}"
EXTRA_ARGS="${EXTRA_ARGS:-}"

# Ports
VLLM_PORT="${VLLM_PORT:-8000}"
RAY_DASHBOARD_PORT="${RAY_DASHBOARD_PORT:-8265}"
RAY_PORT="${RAY_PORT:-6380}"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Auto-detect Network Configuration
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Uses ibdev2netdev to discover active InfiniBand/RoCE interfaces.
# The IP address on the IB/RoCE interface can be any valid IP (not limited
# to link-local addresses). We rely on ibdev2netdev output to identify the
# correct network interface for RDMA communication.
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Discover primary RoCE/IB interface using ibdev2netdev
discover_ib_interface() {
  if command -v ibdev2netdev >/dev/null 2>&1; then
    # Get the first active (Up) interface from ibdev2netdev
    local active_line
    active_line=$(ibdev2netdev 2>/dev/null | awk '/\(Up\)/ {print; exit}')

    if [ -n "$active_line" ]; then
      # Extract interface name (5th field, removing parentheses)
      echo "$active_line" | awk '{print $5}' | tr -d '()'
    fi
  fi
}

# Get all active IB/RoCE HCAs (comma-separated for NCCL_IB_HCA)
discover_all_ib_hcas() {
  if command -v ibdev2netdev >/dev/null 2>&1; then
    ibdev2netdev 2>/dev/null | grep "(Up)" | awk '{print $1}' | sort | tr '\n' ',' | sed 's/,$//'
  fi
}

# Get the first IPv4 address from an interface
get_interface_ip() {
  local iface="$1"
  if [ -n "$iface" ]; then
    ip -o addr show "$iface" 2>/dev/null | grep "inet " | awk '{print $4}' | cut -d'/' -f1 | head -1
  fi
}

# Auto-detect primary IB/RoCE interface
PRIMARY_IB_IF=$(discover_ib_interface)

# Auto-detect HEAD_IP from IB interface (or use override)
if [ -z "${HEAD_IP:-}" ]; then
  if [ -n "${PRIMARY_IB_IF}" ]; then
    HEAD_IP=$(get_interface_ip "${PRIMARY_IB_IF}")
  fi
  # Final fallback if auto-detection fails
  if [ -z "${HEAD_IP:-}" ]; then
    echo "ERROR: Could not auto-detect HEAD_IP from InfiniBand/RoCE interface."
    echo ""
    echo "Please ensure:"
    echo "  1. The InfiniBand/RoCE cable is connected between nodes"
    echo "  2. Run 'ibdev2netdev' to verify IB/RoCE interfaces are Up"
    echo "  3. Check that an IP is assigned to the IB/RoCE interface"
    echo ""
    echo "Then either:"
    echo "  - Fix the interface and re-run this script, OR"
    echo "  - Set HEAD_IP manually: export HEAD_IP=<your_ib_ip>"
    exit 1
  fi
fi

# Auto-detect network interfaces from active IB/RoCE devices
if [ -z "${GLOO_IF:-}" ] || [ -z "${TP_IF:-}" ] || [ -z "${NCCL_IF:-}" ] || [ -z "${UCX_DEV:-}" ]; then
  if [ -n "${PRIMARY_IB_IF}" ]; then
    # Use primary IB interface for all NCCL/GLOO/TP/UCX communication
    GLOO_IF="${GLOO_IF:-${PRIMARY_IB_IF}}"
    TP_IF="${TP_IF:-${PRIMARY_IB_IF}}"
    NCCL_IF="${NCCL_IF:-${PRIMARY_IB_IF}}"
    UCX_DEV="${UCX_DEV:-${PRIMARY_IB_IF}}"
  else
    # Error if no IB interface detected and not manually specified
    echo "ERROR: No active InfiniBand/RoCE interface detected."
    echo "Run 'ibdev2netdev' to check interface status."
    exit 1
  fi
fi

# Auto-detect InfiniBand HCAs using ibdev2netdev (or use override)
if [ -z "${NCCL_IB_HCA:-}" ]; then
  IB_DEVICES=$(discover_all_ib_hcas)
  if [ -n "${IB_DEVICES}" ]; then
    NCCL_IB_HCA="${IB_DEVICES}"
  else
    # Fallback: use all IB devices from sysfs
    IB_DEVICES=$(ls -1 /sys/class/infiniband/ 2>/dev/null | tr '\n' ',' | sed 's/,$//')
    NCCL_IB_HCA="${IB_DEVICES:-}"
    if [ -z "${NCCL_IB_HCA}" ]; then
      echo "ERROR: No InfiniBand HCAs detected."
      echo "Run 'ibdev2netdev' or check /sys/class/infiniband/"
      exit 1
    fi
  fi
fi

# Set OMPI_MCA for MPI-based communication (needed for some frameworks)
OMPI_MCA_IF="${NCCL_IF}"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*"
}

error() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $*" >&2
  exit 1
}

# Step counter - incremented before each step
CURRENT_STEP=0

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Pre-flight Checks
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "Pre-flight Checks"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

PREFLIGHT_FAILED=false

# Check 1: WORKER_HOST must be set for distributed setup
echo "Checking WORKER_HOST..."
if [ -z "${WORKER_HOST}" ]; then
  echo "  âŒ WORKER_HOST is not set"
  echo ""
  echo "  For distributed inference, you must set WORKER_HOST to the worker's"
  echo "  Ethernet IP address (for SSH access)."
  echo ""
  echo "  Run: source ./setup-env.sh --head"
  echo ""
  PREFLIGHT_FAILED=true
else
  echo "  âœ… WORKER_HOST=${WORKER_HOST}"
fi

# Check 2: SSH connectivity to worker
if [ -n "${WORKER_HOST}" ]; then
  echo ""
  echo "Checking SSH connectivity to worker..."
  if ssh -o BatchMode=yes -o ConnectTimeout=5 "${WORKER_USER}@${WORKER_HOST}" "echo 'SSH OK'" >/dev/null 2>&1; then
    echo "  âœ… SSH to ${WORKER_USER}@${WORKER_HOST} successful"
  else
    echo "  âŒ Cannot SSH to ${WORKER_USER}@${WORKER_HOST}"
    echo ""
    echo "  Passwordless SSH must be configured. Run:"
    echo ""
    echo "    ssh-copy-id ${WORKER_USER}@${WORKER_HOST}"
    echo ""
    echo "  Note: You also need passwordless SSH to the worker's InfiniBand IP"
    echo "  for NCCL communication. If your worker has IB IP 169.254.x.x, also run:"
    echo ""
    echo "    ssh-copy-id ${WORKER_USER}@<worker-infiniband-ip>"
    echo ""
    PREFLIGHT_FAILED=true
  fi
fi

# Check 3: Docker available
echo ""
echo "Checking Docker..."
if command -v docker >/dev/null 2>&1; then
  echo "  âœ… Docker is available"
else
  echo "  âŒ Docker is not installed or not in PATH"
  PREFLIGHT_FAILED=true
fi

# Check 4: NVIDIA GPU access
echo ""
echo "Checking NVIDIA GPU access..."
if docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi >/dev/null 2>&1; then
  echo "  âœ… NVIDIA GPU access via Docker is working"
else
  echo "  âŒ Cannot access NVIDIA GPUs via Docker"
  echo "  Ensure nvidia-container-toolkit is installed"
  PREFLIGHT_FAILED=true
fi

# Check 5: InfiniBand detection (already done above, but summarize)
echo ""
echo "Checking InfiniBand..."
if [ -n "${PRIMARY_IB_IF}" ]; then
  echo "  âœ… InfiniBand interface detected: ${PRIMARY_IB_IF}"
  echo "  âœ… Head IP (auto-detected): ${HEAD_IP}"
else
  echo "  âŒ No active InfiniBand interface detected"
  echo "  Run 'ibdev2netdev' to check interface status"
  PREFLIGHT_FAILED=true
fi

# Check 6: Local HuggingFace cache permissions
echo ""
echo "Checking local HuggingFace cache permissions..."
if [ -d "${HF_CACHE}" ]; then
  if [ -w "${HF_CACHE}" ]; then
    echo "  âœ… Local HF cache is writable (${HF_CACHE})"
  else
    echo "  âš ï¸  Local HF cache is not writable: ${HF_CACHE}"
    echo ""
    echo "  Docker containers run as root and may have created files owned by root."
    read -p "  Fix permissions now? (requires sudo) [y/N]: " fix_local_perms
    if [[ "$fix_local_perms" =~ ^[Yy]$ ]]; then
      if sudo chown -R "$USER" "${HF_CACHE}"; then
        echo "  âœ… Local HF cache permissions fixed"
      else
        echo "  âŒ Failed to fix local permissions"
        PREFLIGHT_FAILED=true
      fi
    else
      echo "  âŒ Local HF cache permissions not fixed - rsync may fail"
      PREFLIGHT_FAILED=true
    fi
  fi
else
  echo "  â„¹ï¸  Local HF cache will be created at ${HF_CACHE}"
fi

# Check 7: Remote HuggingFace cache permissions (only if WORKER_HOST is set)
if [ -n "${WORKER_HOST}" ]; then
  echo ""
  echo "Checking remote HuggingFace cache permissions on worker..."
  REMOTE_HF_WRITABLE=$(ssh -o BatchMode=yes -o ConnectTimeout=5 "${WORKER_USER}@${WORKER_HOST}" "
    if [ -d '${WORKER_HF_CACHE}' ]; then
      if [ -w '${WORKER_HF_CACHE}' ]; then
        echo 'writable'
      else
        echo 'not_writable'
      fi
    else
      echo 'not_exists'
    fi
  " 2>/dev/null || echo "ssh_failed")

  case "$REMOTE_HF_WRITABLE" in
    writable)
      echo "  âœ… Remote HF cache is writable (${WORKER_HF_CACHE})"
      ;;
    not_writable)
      echo "  âš ï¸  Remote HF cache is not writable: ${WORKER_HF_CACHE}"
      echo ""
      echo "  Docker containers run as root and may have created files owned by root."
      read -p "  Fix permissions on worker now? (requires sudo password on worker) [y/N]: " fix_remote_perms
      if [[ "$fix_remote_perms" =~ ^[Yy]$ ]]; then
        echo "  Running: sudo chown -R ${WORKER_USER} ${WORKER_HF_CACHE} on ${WORKER_HOST}"
        if ssh -t "${WORKER_USER}@${WORKER_HOST}" "sudo chown -R ${WORKER_USER} '${WORKER_HF_CACHE}'"; then
          echo "  âœ… Remote HF cache permissions fixed"
        else
          echo "  âŒ Failed to fix remote permissions"
          PREFLIGHT_FAILED=true
        fi
      else
        echo "  âŒ Remote HF cache permissions not fixed - rsync may fail"
        PREFLIGHT_FAILED=true
      fi
      ;;
    not_exists)
      echo "  â„¹ï¸  Remote HF cache will be created at ${WORKER_HF_CACHE}"
      ;;
    ssh_failed)
      echo "  âš ï¸  Could not check remote HF cache (SSH failed)"
      ;;
  esac
fi

echo ""

# Exit if any pre-flight check failed
if [ "$PREFLIGHT_FAILED" = true ]; then
  echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  echo "âŒ Pre-flight checks failed. Please fix the issues above and re-run."
  echo ""
  echo "To configure required variables, run:"
  echo "  source ./setup-env.sh --head"
  echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  exit 1
fi

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… All pre-flight checks passed"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Starting DGX Spark vLLM Head Node Setup"
log "Configuration:"
log "  Image:           ${IMAGE}"
log "  Head IP:         ${HEAD_IP} (auto-detected)"
log "  Model:           ${MODEL}"
log "  Tensor Parallel: ${TENSOR_PARALLEL}"
log "  Load Format:     ${LOAD_FORMAT}"
log "  Ray Version:     ${RAY_VERSION}"
log ""
if [ -n "${WORKER_HOST}" ]; then
  log "Worker Node Configuration (orchestrated setup enabled):"
  log "  Worker Host:     ${WORKER_HOST}"
  log "  Worker User:     ${WORKER_USER}"
  log "  Worker HF Cache: ${WORKER_HF_CACHE}"
  log ""
else
  log "Worker Node Configuration:"
  log "  âš ï¸  WORKER_HOST not set - manual worker setup required"
  log "     Set WORKER_HOST=<ip> to enable automatic worker orchestration"
  log ""
fi
log "Network Configuration (auto-detected from ibdev2netdev):"
log "  Primary IB IF:   ${PRIMARY_IB_IF:-<not detected>}"
log "  GLOO Interface:  ${GLOO_IF}"
log "  TP Interface:    ${TP_IF}"
log "  NCCL Interface:  ${NCCL_IF}"
log "  UCX Device:      ${UCX_DEV}"
log "  OMPI MCA IF:     ${OMPI_MCA_IF}"
log "  NCCL IB HCAs:    ${NCCL_IB_HCA}"
log ""
if [ -n "${HF_TOKEN}" ]; then
  log "  HF Auth:        âœ… Token provided"
else
  log "  HF Auth:        âš ï¸  No token (gated models will fail)"
fi
log ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Calculate total steps based on whether worker orchestration is enabled
if [ -n "${WORKER_HOST}" ]; then
  TOTAL_STEPS=12
else
  TOTAL_STEPS=9
fi

log "Step 1/${TOTAL_STEPS}: Pulling Docker image"
if ! docker pull "${IMAGE}"; then
  error "Failed to pull image ${IMAGE}"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 2/${TOTAL_STEPS}: Cleaning old container"
if docker ps -a --format '{{.Names}}' | grep -qx "${NAME}"; then
  log "  Removing existing container: ${NAME}"
  docker rm -f "${NAME}" >/dev/null
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 3/${TOTAL_STEPS}: Starting head container"

# Build environment variable args for IB/NCCL configuration
# These are passed into the container to ensure NCCL uses the IB/RoCE link
ENV_ARGS=(
  -e VLLM_HOST_IP="${HEAD_IP}"
  # IB/RoCE interface settings for NCCL communication
  -e GLOO_SOCKET_IFNAME="${GLOO_IF}"
  -e TP_SOCKET_IFNAME="${TP_IF}"
  -e NCCL_SOCKET_IFNAME="${NCCL_IF}"
  -e UCX_NET_DEVICES="${UCX_DEV}"
  -e OMPI_MCA_btl_tcp_if_include="${OMPI_MCA_IF}"
  # NCCL InfiniBand settings
  -e NCCL_IB_DISABLE=0
  -e NCCL_IB_HCA="${NCCL_IB_HCA}"
  -e NCCL_NET_GDR_LEVEL=5
  # Debug settings (can be disabled for production by setting NCCL_DEBUG=WARN)
  -e NCCL_DEBUG="${NCCL_DEBUG:-INFO}"
  -e NCCL_DEBUG_SUBSYS="${NCCL_DEBUG_SUBSYS:-INIT,NET}"
  # NVIDIA/GPU settings
  -e NVIDIA_VISIBLE_DEVICES=all
  -e NVIDIA_DRIVER_CAPABILITIES=all
  # Ray settings
  -e RAY_memory_usage_threshold=0.998
  -e RAY_GCS_SERVER_PORT=6380
  # HuggingFace cache
  -e HF_HOME=/root/.cache/huggingface
)

# Add HuggingFace token if provided
if [ -n "${HF_TOKEN}" ]; then
  ENV_ARGS+=(-e HF_TOKEN="${HF_TOKEN}")
fi

# Run container as root (required for NVIDIA/vLLM)
# HF cache is mounted to /root/.cache/huggingface
docker run -d \
  --restart unless-stopped \
  --name "${NAME}" \
  --gpus all \
  --network host \
  --shm-size="${SHM_SIZE}" \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  --device=/dev/infiniband \
  -v "${HF_CACHE}:/root/.cache/huggingface" \
  "${ENV_ARGS[@]}" \
  "${IMAGE}" sleep infinity

if ! docker ps | grep -q "${NAME}"; then
  error "Container failed to start"
fi

log "  Container started successfully"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 4/${TOTAL_STEPS}: Verifying RDMA/InfiniBand libraries for NCCL"
log "  These libraries are required for NCCL to use InfiniBand/RoCE instead of Ethernet"
# NVIDIA vLLM container already includes RDMA libraries (libibverbs, librdmacm, ibverbs-providers)
# We just verify they're present rather than installing
if docker exec "${NAME}" bash -lc "ldconfig -p 2>/dev/null | grep -q libibverbs"; then
  log "  âœ… RDMA libraries available (libibverbs, librdmacm)"
else
  log "  âš ï¸  Warning: RDMA libraries may not be properly installed"
  log "     NCCL may fall back to Socket transport (slower)"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 5/${TOTAL_STEPS}: Verifying container dependencies"
# The nvidia vLLM container already has vLLM and Ray properly built
# Do NOT pip install vllm as it would overwrite the CUDA-enabled version with CPU-only PyPI version

# Verify vLLM is available with CUDA
CUDA_AVAILABLE=$(docker exec "${NAME}" python3 -c "import torch; print(torch.cuda.is_available())" 2>/dev/null || echo "False")
if [ "${CUDA_AVAILABLE}" != "True" ]; then
  error "PyTorch CUDA not available - container may be corrupted. Try: docker pull nvcr.io/nvidia/vllm:25.11-py3"
fi
log "  âœ… PyTorch CUDA available"

INSTALLED_VLLM_VERSION=$(docker exec "${NAME}" python3 -c "import vllm; print(vllm.__version__)" 2>/dev/null || echo "unknown")
if [ "${INSTALLED_VLLM_VERSION}" == "unknown" ]; then
  error "vLLM not found in container"
fi
log "  âœ… vLLM ${INSTALLED_VLLM_VERSION} available"

# Verify Ray version
INSTALLED_RAY_VERSION=$(docker exec "${NAME}" python3 -c "import ray; print(ray.__version__)" 2>/dev/null || echo "unknown")
if [ "${INSTALLED_RAY_VERSION}" == "unknown" ]; then
  error "Ray not found in container"
fi
log "  âœ… Ray ${INSTALLED_RAY_VERSION} available"

# Install only fastsafetensors if not already present (does not affect vLLM/PyTorch)
if ! docker exec "${NAME}" python3 -c "import fastsafetensors" 2>/dev/null; then
  log "  Installing fastsafetensors..."
  # Unset PIP_CONSTRAINT to avoid deprecation warning from nvidia container
  docker exec "${NAME}" bash -lc "unset PIP_CONSTRAINT && pip install -q --root-user-action=ignore fastsafetensors" || true
fi
log "  fastsafetensors available (GPU Direct Storage for faster model loading)"

# Apply fastsafetensors cluster patch (fixes device group handling in distributed setup)
# See: https://github.com/foundation-model-stack/fastsafetensors/issues/36
PATCH_FILE="${SCRIPT_DIR}/patches/fastsafetensors.patch"
if [ -f "${PATCH_FILE}" ]; then
  log "  Applying fastsafetensors cluster patch..."
  # Copy patch into container and apply to vLLM's weight_utils.py
  docker cp "${PATCH_FILE}" "${NAME}:/tmp/fastsafetensors.patch"
  if docker exec "${NAME}" bash -lc "
    cd /usr/local/lib/python3.12/dist-packages && \
    patch -p1 --forward < /tmp/fastsafetensors.patch 2>/dev/null || \
    patch -p1 --forward -d /opt/vllm < /tmp/fastsafetensors.patch 2>/dev/null || \
    echo 'Patch may already be applied or path differs'
  "; then
    log "  âœ… fastsafetensors cluster patch applied"
  else
    log "  âš ï¸  Could not apply fastsafetensors patch (may already be applied)"
  fi
else
  log "  âš ï¸  fastsafetensors patch not found at ${PATCH_FILE}"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

log "Step 6/${TOTAL_STEPS}: Pre-downloading model weights"
log "  Model: ${MODEL}"
log "  This may take a while for large models on first download..."

# Build HF token arg if provided
HF_TOKEN_ARG=""
if [ -n "${HF_TOKEN}" ]; then
  HF_TOKEN_ARG="--token ${HF_TOKEN}"
fi

# Download model with verification
if ! docker exec "${NAME}" bash -lc "
  export HF_HOME=/root/.cache/huggingface
  echo '  Downloading model files (excluding original/* and metal/* to save space)...'
  hf download ${MODEL} ${HF_TOKEN_ARG} --exclude 'original/*' --exclude 'metal/*' 2>&1 | tail -5
"; then
  error "Failed to download model ${MODEL}"
fi

# Verify model was downloaded by checking for config.json
if ! docker exec "${NAME}" bash -lc "
  export HF_HOME=/root/.cache/huggingface
  python3 -c \"
from huggingface_hub import snapshot_download
import os
path = snapshot_download('${MODEL}', local_files_only=True)
config_path = os.path.join(path, 'config.json')
if not os.path.exists(config_path):
    raise FileNotFoundError(f'Model config not found at {config_path}')
print(f'  âœ… Model verified at: {path}')
\"
"; then
  error "Model verification failed - config.json not found"
fi

log "  Model download complete and verified"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Worker orchestration steps (only if WORKER_HOST is set)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if [ -n "${WORKER_HOST}" ]; then
  log "Step 7/${TOTAL_STEPS}: Syncing model to worker node"
  log "  Worker host: ${WORKER_USER}@${WORKER_HOST}"
  log "  Source: ${HF_CACHE}/"
  log "  Destination: ${WORKER_HF_CACHE}/"
  log ""
  log "  This may take a while for large models..."

  # Test SSH connectivity first
  if ! ssh -o BatchMode=yes -o ConnectTimeout=5 "${WORKER_USER}@${WORKER_HOST}" "echo 'SSH OK'" >/dev/null 2>&1; then
    error "Cannot SSH to ${WORKER_USER}@${WORKER_HOST}. Please ensure:"$'\n'"  1. SSH keys are set up (ssh-copy-id ${WORKER_USER}@${WORKER_HOST})"$'\n'"  2. The worker host is reachable"
  fi
  log "  âœ… SSH connectivity verified"

  # Ensure destination directory exists and fix permissions if needed
  ssh "${WORKER_USER}@${WORKER_HOST}" "
    mkdir -p ${WORKER_HF_CACHE}/hub
    # Fix ownership if directories exist but are owned by root (from Docker)
    if [ -d '${WORKER_HF_CACHE}' ] && [ ! -w '${WORKER_HF_CACHE}' ]; then
      echo 'Fixing permissions on ${WORKER_HF_CACHE} (requires sudo)...'
      sudo chown -R \$(id -u):\$(id -g) '${WORKER_HF_CACHE}' 2>/dev/null || true
    fi
  "

  # Convert model name to HF cache directory format (e.g., openai/gpt-oss-120b -> models--openai--gpt-oss-120b)
  MODEL_CACHE_NAME="models--$(echo "${MODEL}" | sed 's|/|--|g')"
  MODEL_CACHE_PATH="${HF_CACHE}/hub/${MODEL_CACHE_NAME}"

  if [ ! -d "${MODEL_CACHE_PATH}" ]; then
    error "Model cache not found at ${MODEL_CACHE_PATH}. Download the model first."
  fi

  log "  Syncing model: ${MODEL_CACHE_NAME}"
  log "  From: ${MODEL_CACHE_PATH}"
  log "  To:   ${WORKER_USER}@${WORKER_HOST}:${WORKER_HF_CACHE}/hub/"

  # Rsync only the specific model directory (exclude lock files)
  # Use --no-perms --no-owner --no-group to avoid permission issues with mixed ownership
  if ! rsync -a --info=progress2 --human-readable \
    --no-perms --no-owner --no-group \
    --exclude='.locks' \
    --exclude='*.lock' \
    "${MODEL_CACHE_PATH}" \
    "${WORKER_USER}@${WORKER_HOST}:${WORKER_HF_CACHE}/hub/"; then
    error "Failed to rsync model to worker node"
  fi
  log "  âœ… Model synced to worker"

  # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  log "Step 8/${TOTAL_STEPS}: Copying worker script to worker node"
  WORKER_SCRIPT="${SCRIPT_DIR}/start_worker_vllm.sh"

  if [ ! -f "${WORKER_SCRIPT}" ]; then
    error "Worker script not found at ${WORKER_SCRIPT}"
  fi

  # Copy the worker script to the worker's home directory
  if ! scp "${WORKER_SCRIPT}" "${WORKER_USER}@${WORKER_HOST}:~/start_worker_vllm.sh"; then
    error "Failed to copy worker script to ${WORKER_HOST}"
  fi
  log "  âœ… Worker script copied to ${WORKER_USER}@${WORKER_HOST}:~/start_worker_vllm.sh"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Step number depends on whether worker orchestration steps ran
if [ -n "${WORKER_HOST}" ]; then
  RAY_STEP=9
else
  RAY_STEP=7
fi
log "Step ${RAY_STEP}/${TOTAL_STEPS}: Starting Ray head"
docker exec "${NAME}" bash -lc "
  ray stop --force 2>/dev/null || true
  ray start --head \
    --node-ip-address=${HEAD_IP} \
    --port=${RAY_PORT} \
    --dashboard-host=0.0.0.0 \
    --dashboard-port=${RAY_DASHBOARD_PORT}
" >/dev/null

log "  Ray head started, waiting for readiness..."

# Wait for Ray to become ready
for i in {1..30}; do
  if docker exec "${NAME}" bash -lc "ray status --address='127.0.0.1:${RAY_PORT}' >/dev/null 2>&1"; then
    log "  âœ… Ray head is ready (${i}s)"
    break
  fi
  if [ $i -eq 30 ]; then
    error "Ray head failed to become ready after 30 seconds"
  fi
  sleep 1
done

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if [ -n "${WORKER_HOST}" ]; then
  # Orchestrated mode: Start worker via SSH
  log "Step 10/${TOTAL_STEPS}: Starting worker node via SSH"
  log "  Starting worker script on ${WORKER_USER}@${WORKER_HOST}..."
  log ""

  # Build environment variables to pass to worker
  # Worker needs HEAD_IP, MODEL, HF_TOKEN, and HF_CACHE
  WORKER_ENV="HEAD_IP=${HEAD_IP} MODEL=${MODEL} HF_CACHE=${WORKER_HF_CACHE} RAY_VERSION=${RAY_VERSION} SKIP_MODEL_DOWNLOAD=1"
  if [ -n "${HF_TOKEN}" ]; then
    WORKER_ENV="${WORKER_ENV} HF_TOKEN=${HF_TOKEN}"
  fi

  # Start the worker script via SSH (run in background)
  # Use ssh -f with nohup to properly detach the process
  # The sleep 1 at the end ensures SSH doesn't exit before nohup takes over
  ssh -f "${WORKER_USER}@${WORKER_HOST}" "nohup bash -c '${WORKER_ENV} bash ~/start_worker_vllm.sh > ~/worker_setup.log 2>&1' </dev/null >/dev/null 2>&1 &"

  # Give SSH a moment to start the remote process
  sleep 2

  log "  Worker script started in background on ${WORKER_HOST}"
  log "  Logs available at: ${WORKER_USER}@${WORKER_HOST}:~/worker_setup.log"
  log ""

  # Wait for worker to join the cluster (check for 2+ nodes)
  log "  Waiting for worker to join Ray cluster..."
  WORKER_JOINED=false
  for i in {1..120}; do
    NODE_COUNT=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:${RAY_PORT} 2>/dev/null | grep -E '^ [0-9]+ node_' | wc -l" 2>/dev/null || echo "0")
    CURRENT_GPUS=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:${RAY_PORT} 2>/dev/null | grep 'GPU:' | awk -F'/' '{print \$2}' | awk '{print \$1}'" 2>/dev/null || echo "1")

    if [ "${NODE_COUNT}" -ge 2 ]; then
      echo ""
      log "  âœ… Worker joined cluster (${i}s) - ${NODE_COUNT} nodes, ${CURRENT_GPUS} GPUs"
      WORKER_JOINED=true
      break
    fi

    # Show spinner
    SPINNER="â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â "
    SPIN_CHAR="${SPINNER:$((i % 10)):1}"
    printf "\r  %s Waiting for worker... [%ds elapsed, %s nodes]  " "${SPIN_CHAR}" "${i}" "${NODE_COUNT}"

    sleep 1
  done

  if [ "${WORKER_JOINED}" != "true" ]; then
    echo ""
    log "  âš ï¸  Worker did not join cluster within 120s"
    log "     Check worker logs: ssh ${WORKER_USER}@${WORKER_HOST} 'cat ~/worker_setup.log'"
    log ""
    log "  Proceeding anyway - vLLM may fail if insufficient GPUs"
  fi

else
  # Manual mode: Wait for user to start workers
  log "Step 8/${TOTAL_STEPS}: Waiting for worker nodes"
  log ""
  log "  âš ï¸  IMPORTANT: Set WORKER_HOST to start workers automatically:"
  log "     export WORKER_HOST=<worker_ib_ip>"
  log "     bash start_cluster.sh"
  log ""
  log "  Checking Ray cluster status..."

  # Show current cluster status
  docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:${RAY_PORT} 2>/dev/null | head -15" || true

  CURRENT_NODES=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:${RAY_PORT} 2>/dev/null | grep -E '^ [0-9]+ node' | awk '{print \$1}'" 2>/dev/null || echo "1")
  CURRENT_GPUS=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:${RAY_PORT} 2>/dev/null | grep 'GPU:' | awk -F'/' '{print \$2}' | awk '{print \$1}'" 2>/dev/null || echo "1")

  log ""
  log "  Current cluster: ${CURRENT_NODES} node(s), ${CURRENT_GPUS} GPU(s)"

  if [ "${TENSOR_PARALLEL}" -gt "${CURRENT_GPUS:-1}" ]; then
    log ""
    log "  âš ï¸  Warning: tensor-parallel-size (${TENSOR_PARALLEL}) > available GPUs (${CURRENT_GPUS})"
    log "     Waiting 30 seconds for worker nodes to join..."
    log "     (Press Ctrl+C to abort and add workers manually)"

    for i in {1..30}; do
      CURRENT_GPUS=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:${RAY_PORT} 2>/dev/null | grep 'GPU:' | awk -F'/' '{print \$2}' | awk '{print \$1}'" 2>/dev/null || echo "1")
      if [ "${CURRENT_GPUS:-1}" -ge "${TENSOR_PARALLEL}" ]; then
        log "  âœ… Sufficient GPUs available: ${CURRENT_GPUS}"
        break
      fi
      if [ $i -eq 30 ]; then
        log "  âš ï¸  Proceeding with ${CURRENT_GPUS} GPU(s) - vLLM may fail if insufficient"
      fi
      sleep 1
    done
  fi
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Step number depends on whether worker orchestration was used
if [ -n "${WORKER_HOST}" ]; then
  VLLM_STEP=11
else
  VLLM_STEP=8
fi
log "Step ${VLLM_STEP}/${TOTAL_STEPS}: Starting vLLM server"
log ""

# Kill any existing vLLM processes
docker exec "${NAME}" bash -lc "pkill -f 'vllm serve' 2>/dev/null || true" || true

log "  Starting vLLM in background (this launches the server process)..."

# Build vLLM command arguments
VLLM_ARGS="--distributed-executor-backend ray"
VLLM_ARGS="${VLLM_ARGS} --host 0.0.0.0"
VLLM_ARGS="${VLLM_ARGS} --port ${VLLM_PORT}"
VLLM_ARGS="${VLLM_ARGS} --tensor-parallel-size ${TENSOR_PARALLEL}"
VLLM_ARGS="${VLLM_ARGS} --max-model-len ${MAX_MODEL_LEN}"
VLLM_ARGS="${VLLM_ARGS} --gpu-memory-utilization ${GPU_MEMORY_UTIL}"
VLLM_ARGS="${VLLM_ARGS} --swap-space ${SWAP_SPACE}"
VLLM_ARGS="${VLLM_ARGS} --download-dir /root/.cache/huggingface"
VLLM_ARGS="${VLLM_ARGS} --load-format ${LOAD_FORMAT}"

# Add optional flags
if [ "${ENABLE_EXPERT_PARALLEL}" = "true" ]; then
  VLLM_ARGS="${VLLM_ARGS} --enable-expert-parallel"
fi
if [ "${TRUST_REMOTE_CODE}" = "true" ]; then
  VLLM_ARGS="${VLLM_ARGS} --trust-remote-code"
fi
if [ -n "${EXTRA_ARGS}" ]; then
  VLLM_ARGS="${VLLM_ARGS} ${EXTRA_ARGS}"
fi

# Start vLLM in background using nohup
# Note: We do NOT set HF_HUB_OFFLINE=1 here because workers need to resolve the model name
docker exec "${NAME}" bash -lc "
  export HF_HOME=/root/.cache/huggingface
  export RAY_ADDRESS=127.0.0.1:${RAY_PORT}
  export PYTHONUNBUFFERED=1
  export VLLM_LOGGING_LEVEL=INFO
  export VLLM_MXFP4_USE_MARLIN=1

  nohup vllm serve ${MODEL} ${VLLM_ARGS} > /var/log/vllm.log 2>&1 &

  sleep 1
" || true

log "  vLLM server process started"
log ""
log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
log "  ğŸ”„ MODEL LOADING IN PROGRESS"
log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
log ""
log "  This process typically takes 5-20 minutes depending on model size."
log "  The server will:"
log "    1. Load model weights into GPU memory"
log "    2. Initialize tensor parallelism across GPUs"
log "    3. Compile CUDA graphs for optimized inference"
log ""
log "  Progress updates will appear below..."
log ""

# Wait for vLLM to become ready with detailed progress feedback
VLLM_READY=false
MAX_WAIT=1800  # 30 minutes max for very large models (70B+ can take 15-20+ min)
LAST_STATUS=""
START_TIME=$(date +%s)

for i in $(seq 1 $MAX_WAIT); do
  CURRENT_TIME=$(date +%s)
  ELAPSED=$((CURRENT_TIME - START_TIME))
  MINS=$((ELAPSED / 60))
  SECS=$((ELAPSED % 60))

  # Check if vLLM is ready
  if docker exec "${NAME}" bash -lc "curl -sf http://127.0.0.1:${VLLM_PORT}/health >/dev/null 2>&1"; then
    echo ""
    log ""
    log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    log "  âœ… MODEL LOADED SUCCESSFULLY!"
    log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    log ""
    log "  vLLM is ready and accepting requests (loaded in ${MINS}m ${SECS}s)"
    log ""
    VLLM_READY=true
    break
  fi

  # Check vLLM process status and extract progress from logs
  VLLM_PID=$(docker exec "${NAME}" bash -lc "pgrep -f 'vllm serve' 2>/dev/null" || echo "")

  if [ -z "${VLLM_PID}" ]; then
    # vLLM process died - check logs for error
    echo ""
    log ""
    log "  âŒ vLLM process exited unexpectedly!"
    log ""
    log "  Last 20 lines of vLLM log:"
    docker exec "${NAME}" tail -20 /var/log/vllm.log 2>/dev/null || true
    log ""
    error "vLLM failed to start. Check logs: docker exec ${NAME} cat /var/log/vllm.log"
  fi

  # Parse last meaningful log line to show progress
  CURRENT_STATUS=$(docker exec "${NAME}" bash -lc "tail -50 /var/log/vllm.log 2>/dev/null | grep -E '(Loading|Loaded|weight|layer|graph|CUDA|tensor|parallel|shard|download|progress|INFO)' | tail -1 | sed 's/.*INFO/INFO/' | cut -c1-80" 2>/dev/null || echo "")

  # Show spinner with elapsed time
  SPINNER="â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â "
  SPIN_CHAR="${SPINNER:$((i % 10)):1}"

  # Update progress display
  printf "\r  %s Loading model... [%dm %02ds elapsed]  " "${SPIN_CHAR}" "${MINS}" "${SECS}"

  # Show status updates when they change (avoid flooding terminal)
  if [ -n "${CURRENT_STATUS}" ] && [ "${CURRENT_STATUS}" != "${LAST_STATUS}" ]; then
    echo ""
    log "     ${CURRENT_STATUS}"
    LAST_STATUS="${CURRENT_STATUS}"
  fi

  # Periodic milestone messages
  if [ $((i % 60)) -eq 0 ]; then
    echo ""
    log "  â³ Still loading... (${MINS}m ${SECS}s) - this is normal for large models"
  fi

  sleep 1
done

# Handle timeout
if [ "${VLLM_READY}" != "true" ]; then
  echo ""
  log ""
  log "  âš ï¸  vLLM not ready after $((MAX_WAIT / 60)) minutes"
  log ""
  log "  This could mean:"
  log "    - Model is still loading (very large models may need more time)"
  log "    - An error occurred during loading"
  log ""
  log "  Check the logs for details:"
  log "    docker exec ${NAME} tail -100 /var/log/vllm.log"
  log ""
  log "  You can also monitor GPU memory to see if loading is progressing:"
  log "    watch -n 1 nvidia-smi"
  log ""
fi

log ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Final step is always TOTAL_STEPS
log "Step ${TOTAL_STEPS}/${TOTAL_STEPS}: Running health checks"

# Check Ray status
RAY_NODES=$(docker exec "${NAME}" bash -lc "ray status --address=127.0.0.1:${RAY_PORT} 2>/dev/null | grep 'Healthy:' -A1 | tail -1 | awk '{print \$1}'" || echo "0")
log "  Ray cluster: ${RAY_NODES} node(s) healthy"

# Check vLLM models
VLLM_MODEL=$(docker exec "${NAME}" bash -lc "curl -sf http://127.0.0.1:${VLLM_PORT}/v1/models 2>/dev/null | python3 -c 'import sys,json; print(json.load(sys.stdin)[\"data\"][0][\"id\"])' 2>/dev/null" || echo "unknown")
log "  vLLM model: ${VLLM_MODEL}"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Detect public-facing IP for user access (exclude loopback and docker bridge)
PUBLIC_IP=$(ip -o addr show | grep "inet " | grep -v "127.0.0.1" | grep -v "172.17" | awk '{print $4}' | cut -d'/' -f1 | head -1)
if [ -z "${PUBLIC_IP}" ]; then
  PUBLIC_IP="${HEAD_IP}"  # Fallback to IB IP if no other found
fi

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
if [ -n "${WORKER_HOST}" ]; then
  echo "âœ… Cluster is ready! (Head + Worker orchestrated)"
else
  echo "âœ… Head node is ready!"
fi
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸŒ Services (accessible from network):"
echo "  Ray Dashboard:  http://${PUBLIC_IP}:${RAY_DASHBOARD_PORT}"
echo "  vLLM API:       http://${PUBLIC_IP}:${VLLM_PORT}"
echo ""

if [ -n "${WORKER_HOST}" ]; then
  echo "ğŸ–¥ï¸  Cluster Nodes:"
  echo "  Head:   $(hostname) (${HEAD_IP})"
  echo "  Worker: ${WORKER_HOST}"
  echo ""
  echo "ğŸ”§ Worker Logs:"
  echo "  ssh ${WORKER_USER}@${WORKER_HOST} 'cat ~/worker_setup.log'"
  echo ""
else
  echo "ğŸ”— Next Steps - Start Workers Automatically:"
  echo "  Set WORKER_HOST and re-run start_cluster.sh:"
  echo "    export WORKER_HOST=<worker_ib_ip>"
  echo "    bash start_cluster.sh"
  echo ""
  echo "  Workers will be started automatically via SSH."
  echo "  Head IP for worker communication: ${HEAD_IP}"
  echo ""
fi

echo "ğŸ“Š Quick API Tests:"
echo "  # List models"
echo "  curl http://${PUBLIC_IP}:${VLLM_PORT}/v1/models"
echo ""
echo "  # Chat completion"
echo "  curl http://${PUBLIC_IP}:${VLLM_PORT}/v1/chat/completions \\"
echo "    -H 'Content-Type: application/json' \\"
echo "    -d '{\"model\":\"${MODEL}\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello\"}]}'"
echo ""
echo "ğŸ” Monitoring Commands:"
echo "  # View vLLM logs"
echo "  docker exec ${NAME} tail -f /var/log/vllm.log"
echo ""
echo "  # Ray cluster status (check for worker nodes)"
echo "  docker exec ${NAME} ray status --address=127.0.0.1:${RAY_PORT}"
echo ""
echo "  # GPU utilization"
echo "  watch -n 1 nvidia-smi"
echo ""
echo "âš™ï¸  Current Configuration:"
echo "  Model:              ${MODEL}"
echo "  Tensor Parallelism: ${TENSOR_PARALLEL} GPUs"
echo "  Max Context:        ${MAX_MODEL_LEN} tokens"
echo "  GPU Memory:         ${GPU_MEMORY_UTIL} utilization"
echo "  CUDA Graphs:        Enabled (optimized for performance)"
echo ""
echo "ğŸ“Š Expected Performance:"
echo "  Throughput:         50-100 tokens/second"
echo "  First request:      May be slower (graph warmup)"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
